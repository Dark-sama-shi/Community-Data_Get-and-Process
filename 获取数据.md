# Intro
## About Code

- 数据库：各个城市的链家网站
   - 网络域名：https://城市简写.lianjia.com/
   - 城市简写参考代码块2的city_abbr_dict
- 代码功能：
   - 代码块1：引用包、定义一些全局参数
   - 代码块2：到链家的sitemap网站查看链家提供了哪些城市的信息，将相关信息存到“Results”目录下“城市对应.csv”中。如果只需要其中某些城市的信息，请进入该csv，删掉其他城市对应行，剩下的按照优先级排列。
   - 代码块3：定义get url部分第一个函数，即访问网页并获取小区详细信息网页链接、之后输出信息到Results/href目录下“城市名字_href.csv”的函数（输出的信息有2列：第一列小区名字，第二列小区详细信息网址）。因为链家小区页面每页最多30个、最多30页，所以通过自定义价格范围的方式把搜索结果限制在900以内，保证信息尽量全面
   - 代码块4：定义get url部分第二个函数，即控制各个城市运行代码块3的函数，并添加了若干装饰（包括报错自动返回到适合节点继续运行等）。每个城市搜索输出完毕时、所有城市搜索输出完毕是以及出现错误开始返回到适合节点时会发出提示音（因为代码块3的函数有递归，不太好控制节点，所以失败的话会直接把这个城市输出结果删掉重来，所以如果某个城市的小区数量比较多的话，尽量等这个城市跑完再关程序）
   - 代码块5：运行代码块4的函数
   - 代码块6：整理一下上面的结果，看看链家提供的所有城市里面有哪些真正能通过价格自定义查询的方式获取小区链接等
   - 代码块7：继续整理一下结果，看看真正查到结果的城市里面又有哪些提供了“建筑年代”信息（保存在year_city列表中并输出到Results目录下“year_city.csv”中），哪些没提供（保存在no_year列表中并输出到Results目录下“no_year.csv”中）（报错的话应该是对方服务器暂时出了点问题，重新运行即可）
   - 代码块8：定义get info部分核心函数，即对某个城市按照get url部分取得的小区详情网址查询小区详细信息并输出到Results/info目录下“城市名字_info.csv”。内置了错误时自动返回合适节点重新开始的功能。这部分没有递归，直接遍历就行，所以真的是非常合适的节点，不像代码块4那样删库跑路，可以放心地随时退出
   - 代码块9：控制有建筑年代信息的城市运行代码块8的函数
   - 代码块10：（如果有需要）控制没有建筑年代信息的城市运行代码块8的函数
   - 代码块11：查看当前已经获取了多少小区信息（与代码块6的输出结果比较）
- 代码顺序：
   - 如果从0开始：按顺序依次运行代码块1-10即可（10可以看需要决定是否运行，下同）
   - 如果是一个新的周期需要更新数据：删掉（或备份后删掉）Results目录下的所有文件，按顺序依次运行代码块1-10即可
   - 如果之前完成了“城市对应.csv”的输出，正在进行下一步（但中途退出了）：依次运行代码块1、3-10即可
   - 如果之前完成了“城市对应.csv”和“城市名字_href.csv”的输出，正在进行下一步（但中途退出了）：依次运行代码块1、8-10即可（如果代码块6、7在这个周期之前从来没有运行过的话也需要运行一下，一次就行）
- 代码维护/更新：
   - 现在代码存在的问题/可能的改进方向：
      - 目前访问网页用的都是urllib.request库的相关功能，没有控制时间的参数。用requests可以控制时间，但由于本代码一代作者知识有限，无法解决requests库对链家相关网站尝试访问时返回的ssl error 1129。若能解决此问题则可以控制每次请求访问的时间（按照目前速度来说，选10秒作为超时时间比较好），防止零星几次访问耗费大量时间
      - 目前的代码日志都是直接print到代码运行界面的，如果需要保存日志的话可以考虑输出到txt之类的
      - 目前的代码日志主要包括：访问每个网站需要的时间、当前查询的城市、当前查询的小区、输出的内容、报错/汇报结束等。可以依需要增添删改
      - 链家135个城市只搞到了87个城市的小区信息、20个城市的包含建筑年代的小区信息，其中135到87的过程中有一部分是因为确实没开放小区信息，有一部分是因为没有报价所以无法按照价格自定义来锁定信息到900条以下。可以看看第二部分怎么获取
      - 关于csv：代码里用到的编码都是gb18030，电脑xlsx转存csv的时候用的好像是utf-8（也就是python默认的，不用声明encoding='utf-8'了）。所以尽量一个电脑操作，避免转来转去
   - 代码更新：
      - 如果链家的网络架构发生了变化，需要重新解析相关信息在html代码的什么位置
      - 如果需要别的信息，请自行解析网页并添加到代码中
      - 如果链家相关信息需要登陆才能获取了，查询cookies相关的爬虫技巧并添加到本篇代码
      - 4G移动网络的热点或校园网访问每个网址平均1.2s（肉眼观察）
## Legal

- [某律师总结](https://www.xianjichina.com/news/details_166565.html)，从中知道以下信息：
   - 禁止爬取个人信息
   - 禁止爬取商业机密、半公开信息（区别于公开信息。链家这些信息任何人都可以查询到，属于公开信息）
   - 若相关网站发布了反爬声明（即根网站目录下的“/robots.txt”文件），爬取信息需遵循该声明
   - 尊重知识产权，不得打包出售数据
   - 爬虫程序的访问频率不能超过对方服务器所有用户访问频率累加的1/3（依据链家服务器响应速度来看，我们的访问频率肯定是不超标的）
   - 不能用人家的信息去和人家进行商业竞争
- 链家的[反爬声明](https://bj.lianjia.com/robots.txt/)
   - 本篇代码未访问链家反爬声明中禁止访问的目录
   - 不要通过查询小区名字的方式查询小区，因为反爬声明中禁止访问“/rs”目录
# Get Url
```python
# 1.初始化
from bs4 import BeautifulSoup#用来解析获得的html代码
import os#文件管理
from urllib import request#用来访问网页
from urllib.request import urlopen#用来访问网页
import csv#用来写入csv
import numpy as np#处理数据
import pandas as pd#读入、处理数据
import time#主要用time.time()查看当前时间、监控运行时间
import winsound#提示音
from math import floor, ceil#向下/上取整，用来自定义递归时处理价格
import urllib.parse#用来转码，代码更新后似乎没什么用了
import json#用来解析字符串里的字典，代码更新后似乎没什么用了
from time import sleep as wait#等待，代码更新后似乎没什么用了
from random import *#生成随机数（用来等待），代码更新后似乎没什么用了

def chinese(name):#汉字转url编码，代码更新后似乎没什么用了
    return urllib.parse.quote(name)
#定义请求头，以防被对方网站识别为爬虫程序被墙掉
headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36 Edg/97.0.1072.55'}
#创建一个Results文件夹用来存放结果
if not os.path.exists('Results'):
    os.mkdir('Results')
if not os.path.exists('Results/href'):
    os.mkdir('Results/href')
if not os.path.exists('Results/info'):
    os.mkdir('Results/info')
```
```python
# 2.查看链家提供了哪些城市的子网站
sitemap_url='https://bj.lianjia.com/sitemap/' #链家索引网站
req=request.Request(url=sitemap_url,headers=headers) #定义请求方式
string=request.urlopen(req).read().decode() #请求访问
soup=BeautifulSoup(string,features='html.parser') #soup解析
name_li=[a.findAll('a')[0].contents[0] for a in soup.findAll('li',{'class':"fir_li"})] #链家数据库有的所有城市名字
abbr_li=[a.find('a')['href'].replace('//','').replace('.lianjia.com/sitemap/','') for a in soup.findAll('li',{'class':"fir_li"})] #城市对应简写
city_abbr_dict=dict()#城市名（汉字）和链家简写（英文）的对应字典（在当前代码块的下半部分写入信息），没什么用，可以拎出来查看一下
# 下面输入到表格
try:#如果原来有先删了
    os.remove('Results/城市对应.csv')
except:
    pass
output=open('Results/城市对应.csv','a',newline='',encoding='gb18030')
csv_write=csv.writer(output,dialect='excel')
csv_write.writerow(('城市','简写'))
for i in range(len(name_li)):
    csv_write.writerow((name_li[i],abbr_li[i]))
    city_abbr_dict[name_li[i]]=abbr_li[i]
output.close()
print('写入完毕')
```
```python
# 3.定义一个函数，输入城市名，就开始按价格自定义查询小区信息，超过900就一分为2继续查询（注意把小数处理为整数，整数互相拆分）输出两列：小区名字，网址
def find_900(start_time,city_name,city_abbr,price_floor,price_ceil):#价格是整数，单位是元
    a_time=time.time()
    print('当前搜索%s价格在%i以上%i以下的小区信息，网址为：'%(city_name,price_floor,price_ceil))
    #发现西安的子网站会对“0.00”之类的查询值产生不理解，所以还是得精确保留小数位数啊
    if price_floor%10000==0:
        if price_ceil%10000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.0fep%.0f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%1000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.0fep%.1f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%100==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.0fep%.2f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%10==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.0fep%.3f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        else:
            url='https://%s.lianjia.com/xiaoqu/bp%.0fep%.4f/'%(city_abbr,price_floor/10000,price_ceil/10000)
    elif price_floor%1000==0:
        if price_ceil%10000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.1fep%.0f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%1000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.1fep%.1f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%100==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.1fep%.2f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%10==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.1fep%.3f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        else:
            url='https://%s.lianjia.com/xiaoqu/bp%.1fep%.4f/'%(city_abbr,price_floor/10000,price_ceil/10000)
    elif price_floor%100==0:
        if price_ceil%10000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.2fep%.0f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%1000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.2fep%.1f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%100==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.2fep%.2f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%10==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.2fep%.3f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        else:
            url='https://%s.lianjia.com/xiaoqu/bp%.2fep%.4f/'%(city_abbr,price_floor/10000,price_ceil/10000)
    elif price_floor%10==0:
        if price_ceil%10000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.3fep%.0f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%1000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.3fep%.1f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%100==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.3fep%.2f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%10==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.3fep%.3f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        else:
            url='https://%s.lianjia.com/xiaoqu/bp%.3fep%.4f/'%(city_abbr,price_floor/10000,price_ceil/10000)
    else:
        if price_ceil%10000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.4fep%.0f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%1000==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.4fep%.1f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%100==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.4fep%.2f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        elif price_ceil%10==0:
            url='https://%s.lianjia.com/xiaoqu/bp%.4fep%.3f/'%(city_abbr,price_floor/10000,price_ceil/10000)
        else:
            url='https://%s.lianjia.com/xiaoqu/bp%.4fep%.4f/'%(city_abbr,price_floor/10000,price_ceil/10000)
    print(url)
    try:
        req=request.Request(url=url,headers=headers) #定义请求方式
        string=request.urlopen(req).read().decode() #请求访问
        print('访问当前网页消耗时间为%.2f秒'%(time.time()-a_time))
    except:
        a_time=time.time()
        print('第一次尝试失败，进行第二次尝试')
        req=request.Request(url=url,headers=headers) #定义请求方式
        string=request.urlopen(req).read().decode() #请求访问
        print('访问当前网页消耗时间为%.2f秒'%(time.time()-a_time))
    soup=BeautifulSoup(string,features='html.parser') #soup解析
    try:
        community_num=int(soup.findAll('h2',{'class':"total fl"})[0].findAll('span')[0].contents[0])
    except:
        print('该城市无小区信息或无法通过自定义价格查询')
        return
    if community_num>900:
        print('当前范围有%i个小区，超过900，开始递归'%community_num)
        if time.time()-start_time<60:
            print('本轮共运行了%.2f秒'%(time.time()-start_time))
        else:
            print('本轮共运行了%.2f分钟'%((time.time()-start_time)/60))
        find_900(start_time,city_name,city_abbr,price_floor,floor((price_floor+price_ceil)/2+0.1))
        find_900(start_time,city_name,city_abbr,ceil((price_floor+price_ceil)/2+0.1),price_ceil)
        return
    elif community_num==0:
        print('当前范围无小区')
        return
    else:#开始输出
        print('当前范围有%i个小区'%community_num)
        now_num=len(soup.findAll('div',{'class':"info"}))
        for j in range(now_num):
            output=open('Results/href/%s_href.csv'%city_name,'a',newline='',encoding='gb18030')
            csv_write=csv.writer(output,dialect='excel')
            csv_write.writerow((soup.findAll('div',{'class':"info"})[j].find('a',{'target':"_blank"}).contents[0],soup.findAll('div',{'class':"info"})[j].find('a',{'target':"_blank"})['href']))
            output.close()
            print((soup.findAll('div',{'class':"info"})[j].find('a',{'target':"_blank"}).contents[0],soup.findAll('div',{'class':"info"})[j].find('a',{'target':"_blank"})['href']),'已输出')
        print('第1/%i页输出完毕'%ceil(community_num/30))
        if time.time()-start_time<60:
            print('本轮共运行了%.2f秒'%(time.time()-start_time))
        else:
            print('本轮共运行了%.2f分钟'%((time.time()-start_time)/60))
        for j in range(1,ceil(community_num/30)):
            a_time=time.time()
            print('进入第%i/%i页，网址为：'%(j+1,ceil(community_num/30)))
            print(url+'pg%i/'%(j+1))
            try:
                req=request.Request(url=url+'pg%i'%(j+1),headers=headers)
                string=request.urlopen(req).read().decode()
                print('访问当前网页消耗时间为%.2f秒'%(time.time()-a_time))
            except:
                a_time=time.time()
                print('第一次尝试失败，进行第二次尝试')
                req=request.Request(url=url+'pg%i'%(j+1),headers=headers)
                string=request.urlopen(req).read().decode()
                print('访问当前网页消耗时间为%.2f秒'%(time.time()-a_time))
            soup=BeautifulSoup(string,features='html.parser')
            now_num=len(soup.findAll('div',{'class':"info"}))
            for k in range(now_num):
                output=open('Results/href/%s_href.csv'%city_name,'a',newline='',encoding='gb18030')
                csv_write=csv.writer(output,dialect='excel')
                csv_write.writerow((soup.findAll('div',{'class':"info"})[k].find('a',{'target':"_blank"}).contents[0],soup.findAll('div',{'class':"info"})[k].find('a',{'target':"_blank"})['href']))
                output.close()
                print((soup.findAll('div',{'class':"info"})[k].find('a',{'target':"_blank"}).contents[0],soup.findAll('div',{'class':"info"})[k].find('a',{'target':"_blank"})['href']),'已输出')
            print('第%i/%i页输出完毕'%(j+1,ceil(community_num/30)))
            if time.time()-start_time<60:
                print('本轮共运行了%.2f秒'%(time.time()-start_time))
            else:
                print('本轮共运行了%.2f分钟'%((time.time()-start_time)/60))
        print('当前搜索结果输出完毕')
        return
```
```python
# 4.定义一个函数，对各个城市运行上面的函数，并加入相关控制功能 以便在出错时自动在合适节点重新运行
def all_country_community_info(start_time):
    try:
        abbr_df=pd.read_csv('Results/城市对应.csv',encoding='gb18030')
        for i in range(len(abbr_df.iloc[:,0])):
            try:
                city_df=pd.read_csv('Results/href/%s_href.csv'%abbr_df.iloc[i,0],encoding='gb18030')
                if city_df.iloc[-1,0]=='输出完毕':
                    print('%s之前已输出完毕'%abbr_df.iloc[i,0])
                    continue
                else:
                    os.remove('Results/href/%s_href.csv'%abbr_df.iloc[i,0])
                    print('删库跑路')
                    output=open('Results/href/%s_href.csv'%abbr_df.iloc[i,0],'a',newline='',encoding='gb18030')
                    csv_write=csv.writer(output,dialect='excel')
                    csv_write.writerow(('小区名称','小区链接'))
                    output.close()
                    find_900(start_time,abbr_df.iloc[i,0],abbr_df.iloc[i,1],0,1000000)
                    output=open('Results/href/%s_href.csv'%abbr_df.iloc[i,0],'a',newline='',encoding='gb18030')
                    csv_write=csv.writer(output,dialect='excel')
                    csv_write.writerow(('输出完毕',))
                    output.close()
                    winsound.MessageBeep()
                    print('%s已输出完毕'%abbr_df.iloc[i,0])
            except:
                output=open('Results/href/%s_href.csv'%abbr_df.iloc[i,0],'a',newline='',encoding='gb18030')
                csv_write=csv.writer(output,dialect='excel')
                csv_write.writerow(('小区名称','小区链接'))
                output.close()
                find_900(start_time,abbr_df.iloc[i,0],abbr_df.iloc[i,1],0,1000000)
                output=open('Results/href/%s_href.csv'%abbr_df.iloc[i,0],'a',newline='',encoding='gb18030')
                csv_write=csv.writer(output,dialect='excel')
                csv_write.writerow(('输出完毕',))
                output.close()
                winsound.MessageBeep()
                print('%s已输出完毕'%abbr_df.iloc[i,0])
        print('全部输出完毕')
        winsound.MessageBeep()
        return
    except:
        print('遇到未知错误，重来')
        winsound.MessageBeep()
        all_country_community_info(start_time)
        return
```
```python
# 5.运行这部分
all_country_community_info(time.time())
```
# Get Info
```python
# 6.进行一些数据处理
city_name=[name[:-9] for name in os.listdir('Results/href') if '_href.csv' in name]
href=[(i,pd.read_csv('Results/href/%s_href.csv'%i,encoding='gb18030').iloc[0,1]) for i in city_name if type(pd.read_csv('Results/href/%s_href.csv'%i,encoding='gb18030').iloc[0,1])==type('a')]
length=[len(pd.read_csv('Results/href/%s_href.csv'%i,encoding='gb18030').iloc[:,0])-1 for i in city_name]
print('当前%i个城市共有%i条信息'%(len(href),sum(length)))
```
```python
# 7.查看各个城市的链家网站都有什么信息
try:
    os.remove('Results/year_city.csv')
    os.remove('Results/no_year.csv')
except:
    pass
output=open('Results/year_city.csv','a',newline='',encoding='gb18030')
csv_write=csv.writer(output,dialect='excel')
csv_write.writerow(['city','parameter'])
output.close()
output=open('Results/no_year.csv','a',newline='',encoding='gb18030')
csv_write=csv.writer(output,dialect='excel')
csv_write.writerow(['city','parameter'])
output.close()
year_city=[]
no_year=[]
for i in href:
    req=request.Request(url=i[1],headers=headers)
    string=request.urlopen(req).read().decode()
    soup=BeautifulSoup(string,features='html.parser')
    if [a.contents[0] for a in soup.findAll('span',{'class':"xiaoquInfoLabel"})][0]=='建筑年代':
        year_city.append((i[0],[a.contents[0] for a in soup.findAll('span',{'class':"xiaoquInfoLabel"})]))
        output=open('Results/year_city.csv','a',newline='',encoding='gb18030')
        csv_write=csv.writer(output,dialect='excel')
        csv_write.writerow([year_city[-1][0],str(year_city[-1][1])])
        output.close()
    else:
        no_year.append((i[0],[a.contents[0] for a in soup.findAll('span',{'class':"xiaoquInfoLabel"})]))
        output=open('Results/no_year.csv','a',newline='',encoding='gb18030')
        csv_write=csv.writer(output,dialect='excel')
        csv_write.writerow([no_year[-1][0],str(no_year[-1][1])])
        output.close()
    print('%s相关已输出；'%i[0],end='')
print('')
length=[len(pd.read_csv('Results/href/%s_href.csv'%i[0],encoding='gb18030').iloc[:,0])-1 for i in year_city]
print('有建筑年代信息的城市有%i个，共有%i条信息'%(len(year_city),sum(length)))
```
```python
# 8.函数：输入（起始时间、）城市名，读入href.csv，尝试读入info.csv，如果没有就写入表头、从头访问，如果有就看看长度、从没输入的下一个位置开始访问
def city_detail(start_time,city_name,city_head):
    try:
        href_df=pd.read_csv('Results/href/%s_href.csv'%city_name,encoding='gb18030')
    except:
        print('未获取到%s小区链接信息'%city_name)
        winsound.MessageBeep()
        return
    try:#看看有没有输出表格
        city_info=pd.read_csv('Results/info/%s_info.csv'%city_name,encoding='gb18030')
    except:#如果没有先弄个表头
        output=open('Results/info/%s_info.csv'%city_name,'a',newline='',encoding='gb18030')
        csv_write=csv.writer(output,dialect='excel')
        temp_li=['小区名称','地址','参考价格']
        temp_li.extend(city_head[:-1])
        csv_write.writerow(temp_li)
        output.close()
        city_detail(start_time,city_name,city_head)
        return
    if len(city_info)==len(href_df)-1:
        print('%s信息之前已查询完毕'%city_name)
        print('-----------------------------------------------------------------------------------------------------------------------------------')
        return
    for i in range(len(city_info),len(href_df)-1):
        a_time=time.time()
        try:
            print('当前访问小区为“%s”，为%s的第%i/%i个，网页为：'%(href_df.iloc[i,0],city_name,i+1,len(href_df)-1))
            print(href_df.iloc[i,1])
            req=request.Request(url=href_df.iloc[i,1],headers=headers) #定义请求方式
            string=request.urlopen(req).read().decode() #请求访问
            print('访问当前网页耗费时间为%.2f秒'%(time.time()-a_time))
        except:
            print('访问网页失败，再次尝试')
            city_detail(start_time,city_name,city_head)
            return
        if '推荐了' in string:
            print('未找到当前小区信息')
            output=open('Results/info/%s_info.csv'%city_name,'a',newline='',encoding='gb18030')
            csv_write=csv.writer(output,dialect='excel')
            csv_write.writerow([href_df.iloc[i,0],'未找到当前小区信息'])
            output.close()
        else:
            soup=BeautifulSoup(string,features='html.parser') #soup解析
            out_list=[href_df.iloc[i,0]]
            try: #获取地址
                out_list.append(soup.findAll('div',{'class':'detailDesc'})[0].contents[0])
            except:
                out_list.append('当前网页找不到元素（地址）')
                print('当前网页找不到元素（地址）')
            try: #获取参考价格
                out_list.append(soup.findAll('span',{'class':'xiaoquUnitPrice'})[0].contents[0])
            except:
                out_list.append('当前网页找不到元素（价格）')
                print('当前网页找不到元素（价格）')
            try: #获取建筑年代等
                out_list.extend((a.contents[0] for a in soup.findAll('span',{'class':'xiaoquInfoContent'})[:-1]))
            except:
                out_list.append('当前网页找不到元素（建筑年代等）')
            output=open('Results/info/%s_info.csv'%city_name,'a',newline='',encoding='gb18030')
            csv_write=csv.writer(output,dialect='excel')
            csv_write.writerow(out_list)
            output.close()
            print(out_list,'已输出')
        if time.time()-start_time<60:
            print('本城市共运行了%.2f秒'%(time.time()-start_time))
        else:
            print('本城市共运行了%.2f分钟'%((time.time()-start_time)/60))
        print('-----------------------------------------------------------------------------------------------------------------------------------')
    winsound.MessageBeep()
    print('%s的小区信息查询并输出完毕'%city_name)
    return
```
## With Year Info
```python
# 9.访问有建成年份信息的城市小区信息
year_df=pd.read_csv('Results/year_city.csv',encoding='gb18030')
year_city=[(year_df.iloc[i,0],eval(year_df.iloc[i,1])) for i in range(len(year_df))]
for i in range(len(year_city)):
    print('开始查询%s的小区信息'%year_city[i][0])
    city_detail(time.time(),year_city[i][0],year_city[i][1])
```
## Without Year Info
```python
# 10.（如果需要）访问没有建成年份信息的城市小区信息
noyear_df=pd.read_csv('Results/no_year.csv',encoding='gb18030')
no_year=[(noyear_df.iloc[i,0],eval(noyear_df.iloc[i,1])) for i in range(len(noyear_df))]
for i in range(len(no_year)):
    print('开始查询%s的小区信息'%no_year[i][0])
    city_detail(time.time(),no_year[i][0],no_year[i][1])
```
## Progress Control
```python
# 11.看看进行到哪了
city_name=[name[:-9] for name in os.listdir('Results/info') if '_info.csv' in name]
length=[len(pd.read_csv('Results/info/%s_info.csv'%i,encoding='gb18030').iloc[:,0])-1 for i in city_name]
print('当前%i个城市共有%i条信息'%(len(city_name),sum(length)))
```
